---
title: 'Graph for LLM (1)'
date: 2025-10-25
permalink: /posts/2025/10/blog-post-1/
# tags:
#   - cool posts
#   - category1
#   - category2
---
Chinese version is in Zhihu [‰∏≠ÊñáÁâà](https://zhuanlan.zhihu.com/p/1962063445030602451)

<!-- This is a sample blog post. Lorem ipsum I can't remember the rest of lorem ipsum and don't have an internet connection right now. Testing testing testing this blog post. Blog posts are cool. -->

Graph for LLM (1)
======

<!-- # Two Papers in discussing graphs on LLMs. -->

Big thanks to **NeurIPS** for accepting my paper,  
**[From Sequence to Structure: Uncovering Substructure Reasoning in Transformers](https://arxiv.org/abs/2507.10435)** ‚Äî otherwise, there‚Äôd be nothing new to post this season.

Honestly, I didn‚Äôt expect it to get in. After the rebuttal, everyone‚Äôs scores went up ‚Äî except mine.  
But I still wanted to go to the conference, so in August I wrote a workshop paper for Efficient Reasoning,  
**[Uncovering Graph Reasoning in Decoder-only Transformers with Circuit Tracing](https://arxiv.org/abs/2509.20336)**.

Luckily, both were accepted in the end ‚Äî huge thanks to the ACs! üôè  See you in San Diego!üïäÔ∏è

---

## Neurips paper on understanding pattern extraction task

Both papers explore how decoder-only Transformers trained from scratch handle graph reasoning tasks.

This question isn‚Äôt new. People have debated for years why Transformers can perform things like path reasoning, such as shortest-path prediction.  
Some argue Transformers memorize edges, others that they ‚Äústack‚Äù edges during search [[2](https://arxiv.org/pdf/2412.04703)].

Most of these studies use simple tasks to probe structural understanding ‚Äî showing that LLMs partially grasp structure, but not fully.

In our earlier work, though, we found something interesting: when Transformers solve graph tasks, they don‚Äôt seem to reason like humans at all.  
So we retrained Transformers from scratch on pattern understanding ‚Äî essentially structure extraction.

But ‚Äústructure extraction‚Äù in LLMs isn‚Äôt just ‚Äúdoes the graph contain this subgraph?‚Äù ‚Äî the model often needs to generate list of nodes to represent each substructure.

To simplify analysis, we used directed graphs, requiring the model to list all nodes in each target subgraph.  
We also minimized textual noise by designing structured inputs:

- **Encoder_G(G)** ‚Üí encodes graph topology (neighbor-based (Adjacency List) or edge-based (Edge List))  
- **Encoder_T** ‚Üí describes the target subgraph (by name or pattern terms)  
- **Encoder_A** ‚Üí encodes the answer (the actual node list for substrcutres)

<!-- ![1](/images/graph_for_llm1/n1.png) -->
<img src="/images/graph_for_llm1/n1.png" width="400"/>
---

<!-- ## 2. Can Transformers Solve Subgraph Tasks? -->

We broke it down systematically:

- Graphs with one target substructure ‚Üí solvable  
- Graphs with multiple substructures of one type ‚Üí solvable  
- Graphs with multiple substructures of different types ‚Üí still solvable  

Why? Because each Transformer layer behaves like an Intra-Sample Fusion (ISF) process ‚Äî each layer absorbs at least one node in parallel, merging multiple subgraph signals simultaneously.

<!-- ![2](/images/graph_for_llm1/n2.png) -->
<img src="/images/graph_for_llm1/n2.png" width="300"/>

We can even observe this at the last token‚Äôs hidden state ‚Äî the aggregated graph representation:

<!-- ![3](/images/graph_for_llm1/n3.png) -->
<img src="/images/graph_for_llm1/n3.png" width="600"/>

By layer 4, the aggregation of nodes (different colors = different node IDs) becomes clearly visible.

And importantly, we‚Äôre looking at the final representation, not embeddings of predicted node IDs ‚Äî meaning the model *already knows the answer* before generation.  
The node tokens are just a way to unpack that hidden structure.

We also tested this on Llama, generating triangle subgraphs:

<!-- ![4](/images/graph_for_llm1/n4.png) -->
<img src="/images/graph_for_llm1/n4.png" width="300"/>

Same story: node aggregation emerges in higher layers.  
So ‚Äî maybe all that ‚Äúreasoning‚Äù and code generation LLMs do? Could just be a *performance*.  
The model already decided where the subgraph is long before it starts ‚Äúthinking.‚Äù in reasoning or coding.

---

<!-- ## 3. Decomposing Structures: How Transformers Simplify Complex Graphs -->

In our previous [GraphPatt](https://arxiv.org/abs/2410.05298) work, we noticed Transformers often decompose complex subgraphs ‚Äî  
e.g., a ‚Äúhouse‚Äù shape (triangle + square) is solved by first finding all triangles, then connecting them.

We verified this by training Transformers on decomposed tasks ‚Äî efficiency shot up.  
Essentially, Transformers detect multiple shapes in parallel across subspaces, and combine them into the final structure.

<!-- ![5](/images/graph_for_llm1/n5.png) -->
<img src="/images/graph_for_llm1/n5.png" width="500"/>

We also tried simple chain-of-thought prompting, but it didn‚Äôt help ‚Äî so maybe pattern-based chains are a more efficient alternative.

And yes, we tried molecular graphs too ‚Äî and surprisingly, it worked even better, since molecule graphs are often sparser.

<!-- ![6](/images/graph_for_llm1/n6.png) -->
<img src="/images/graph_for_llm1/n6.png" width="300"/>

---

## Workshop paper for circuit tracing on graphs

Right in the middle of this, Anthropic dropped  
[On the Biology of a Large Language Model](https://transformer-circuits.pub/2025/attribution-graphs/biology.html) ‚Äî a masterpiece on circuit tracing.

We thought: if they‚Äôre talking about *graphs inside LLMs*, how different are those *implicit* graphs from our *explicit* input graphs?

Since we‚Äôd already studied both path reasoning and pattern extraction, we needed a unified understanding instead of one understanding for one task. Both tasks suggest that LLMs are able to understand the structures, the mechanisms can't be too different ‚Äî  so we directly applied circuit tracing to graph inputs.  
That became our Efficient Reasoning workshop paper.

![7](/images/graph_for_llm1/n7.png)

And the results?

Path reasoning:

<!-- ![8](/images/graph_for_llm1/n8.png) -->
<img src="/images/graph_for_llm1/n8.png" width="300"/>

Patten extraction

<img src="/images/graph_for_llm1/n9.png" width="600"/>

---

Decoder-only Transformers learn graph structures by token merging ‚Äî they compress related information directly. For path reasoning, what gets merged are edges. For pattern extraction, it‚Äôs node groups forming particular shapes  

So edges are basically 2-node patterns, explaining why 2 layers are enough for simple path reasoning.

Then why does it feel hard sometimes?  Suggested in precious work, the reason is that next token prediction is affected by graph density [[3](https://arxiv.org/pdf/2403.06963)].

<!-- ![10](/images/graph_for_llm1/n10.png) -->
<img src="/images/graph_for_llm1/n10.png" width="500"/>

We found that denser graphs push edge aggregation to higher layers meaning layers act like compression units.  
If too many next-token candidates exist (i.e., high density), one layer can‚Äôt hold all the information,  
so more layers are needed.(Compression ratios for future work)

---


We tried to directly align implicit and explicit graph structures ‚Äî and failed.
So no, Transformer reasoning isn‚Äôt human reasoning.  

But the implicit graph inside the model is still useful.
We can, for example, run degree counting or PageRank on it to find influential neuron nodes, as in [GraphGhost](https://arxiv.org/pdf/2510.08613) [[4](https://arxiv.org/pdf/2510.08613)].

Here‚Äôs an interesting case study:

![11](/images/graph_for_llm1/n11.png)

After muting high-degree tokens, Qwen switched to reasoning in Chinese. And yes, people are already running GNNs inside LLMs [[5](https://arxiv.org/pdf/2510.09312)].

---

## Do We Still Need Graph Research in the LLM Era?

Absolutely, graphs are clean, interpretable, and minimal ‚Äî perfect for testing hypotheses about Transformer mechanisms.

They help us probe ideas like:

Superposition for Chains of continuous thought [[6](https://arxiv.org/pdf/2505.12514)], understanding reinforcement learning dynamics [[7](https://arxiv.org/pdf/2509.22613)] or viewing reasoning steps as graph structures [[8](https://arxiv.org/pdf/2506.05744)]. Although step-wise segmentation isn‚Äôt always precise ‚Äî sometimes specific tokens in higher layers trigger logical shifts ‚Äî graphs still give us the clearest window into those processes.

And if we believe an LLM‚Äôs internals resemble a knowledge graph,  
then we can design message-passing mechanisms to make generation more controllable.
The cycle continues:

> Data driven Deep Learning ‚Üí Enhance by Structure data (knowledge controlling / GNN) ‚Üí LLM ‚Üí Knowledge (again, via RAG‚Ä¶ or something next)

---

üïäÔ∏è **See you all in San Diego ‚Äî come find the pigeon poster.**

# Reference:

[1][ALPINE: Unveiling The Planning Capability of Autoregressive Learning in Language Models](https://arxiv.org/pdf/2405.09220), Neurips 2024

[2][TRANSFORMERS STRUGGLE TO LEARN TO SEARCH](https://arxiv.org/pdf/2412.04703), ICLR 2025

[3][The Pitfalls of Next-Token Prediction](https://arxiv.org/pdf/2403.06963), ICML 2024

[4][GraphGhost: Tracing Structures Behind Large Language Models](https://arxiv.org/pdf/2510.08613), arxiv 2025

[5][Verifying Chain-of-Thought Reasoning via Its Computational Graph](https://arxiv.org/pdf/2510.09312), arxiv, 2025

[6][Reasoning by Superposition: A Theoretical Perspective on Chain of Continuous Thought](https://arxiv.org/pdf/2505.12514), Neurips 2025

[7][ BENEFITS AND PITFALLS OF REINFORCEMENT LEARNING FOR LANGUAGE MODEL PLANNING: A THEORETICAL PERSPECTIVE](https://arxiv.org/pdf/2509.22613) arxiv, 2025
 
[8][Topology of Reasoning: Understanding Large Reasoning Models through Reasoning Graph Properties](https://arxiv.org/pdf/2506.05744), Neurips 2025

======

<!-- Aren't headings cool? -->
------